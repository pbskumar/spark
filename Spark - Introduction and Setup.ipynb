{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Apache Spark\n",
    "\n",
    "Apache Spark is a fast and general-purpose open source cluster computing framework. Spark has a Core Engine which manages of Distributed task dispatching, scheduling etc, and provides high-level APIs (supports Scala, Python, R) for implementation. \n",
    "\n",
    "The APIs provieded are centered on Sparks's native data structure known as Resilient Distributed Dataset (RDD). RDDs are immutable data-structures which are inherently fault tolerant. RDDs will be described in detail in later sections.\n",
    "\n",
    "Spark requires a cluster manager and a distributed storage system to implement its core funtionalities. Generally, Spark uses HDFS and YARN of Hadoop 2.x to support its distributed storage and cluster management. Spark also supports standalone local cluster (With just one thread). However, it can be used to interface Cassandra, Amazon S3 for storage and Apache Mesos for cluster management. \n",
    "\n",
    "Spark's Python API, PySpark is used to program Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Installation\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. Install latest version of JAVA.\n",
    "2. Set JAVA_HOME environment variable, pointing to the Java directory\n",
    "\n",
    "   For example, your path could be: C:\\Program Files\\Java\\jdk1.8.0_91\n",
    "3. Download latest Hadoop version from [here](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.1/).\n",
    "4. Set HADOOP_HOME environment variable pointing to Hadoop directory\n",
    "5. Download *winutils.exe* from [here](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin).\n",
    "6. Copy winutils.exe to the bin folder in Hadoop directory (HADOOP_HOME/bi).\n",
    "7. Download Apache Spark pre-built for your Hadoop version from [here](http://spark.apache.org/downloads.html).\n",
    "8. Set SPARK_HOME environment variable pointing to Hadoop directory.\n",
    "9. This is a good time to restart your computer.\n",
    "\n",
    "##### Running Spark\n",
    "10. Now open the command prompt, navigate to SPARK_HOME/bin and execute pyspark.exe\n",
    "    This initializes PySpark and the command prompt must look as shown below. \n",
    "    \n",
    "    ![Here](https://github.com/pbskumar/spark/blob/master/images/pyspark_init.JPG?raw=true)\n",
    "    \n",
    "    \n",
    "    \n",
    "11. Instead of navigating to SPARK_HOME/bin everytime, it is a good idea to add [SPARK_HOME/bin/pyspark.exe) to the PATH environment variable. This way we can run pyspark from any folder.\n",
    "\n",
    "\n",
    "\n",
    "### Linux (Ubuntu)\n",
    "\n",
    "1. Install openSSH for remote access\n",
    "   \n",
    "   `sudo apt-get install openssh-server`\n",
    "2. Create a folder named *software* and navigate to it.\n",
    "   \n",
    "   `mkdir ~/software`\n",
    "   \n",
    "   `cd ~/software`\n",
    "3. Create a backup of `.bashrc` file\n",
    "\n",
    "\n",
    "    `cp ~/.bashrc ~software/bashrc_original`\n",
    "4. Download Java. \n",
    "    `wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u92-b14/jdk-8u92-linux-x64.tar.gz`\n",
    "5. Download Hadoop 2. From [here](http://www-eu.apache.org/dist/hadoop/common/) or using the following script\n",
    "\n",
    "    `wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz`\n",
    "6. Download Spark\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "To be update soon.\n",
    "[Reference](https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Spark\n",
    "\n",
    "#### Windows\n",
    "\n",
    "1. As mentioned earlier, we can directly use the interactive *pyspark.exe* shell.\n",
    "2. Instead, we could use Jupyter Notebook or any IDE (PyCharm)\n",
    "   The script to initialize spark is given below:\n",
    "   ```python\n",
    "   import findspark\n",
    "   findspark.init()\n",
    "   \n",
    "   try:\n",
    "       from pyspark import SparkContext\n",
    "       from pyspark import SparkConf\n",
    "\n",
    "   except ImportError as e:\n",
    "       print(\"Error: \", e)\n",
    "       sys.exit(1)\n",
    "        \n",
    "   conf = SparkConf()\n",
    "   conf.setMaster(\"local\")\n",
    "   conf.setAppName(\"spark_wc\")\n",
    "   \n",
    "   sc = SparkContext(conf=conf)\n",
    "   ```\n",
    "   Note: If `findspark` is not found install it using the following command\n",
    "   `pip install findspark`\n",
    "   \n",
    "3. Another script which can initialize Spark is:\n",
    "    ```python\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    spark_home_folder = os.environ['SPARK_HOME']\n",
    "    sys.path.append(spark_home_folder + r'\\python')\n",
    "\n",
    "    try:\n",
    "        from pyspark import SparkContext\n",
    "        from pyspark import SparkConf\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(\"Error: \", e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster(\"local\")\n",
    "    conf.setAppName(\"spark_wc\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    ```\n",
    "    \n",
    "4. Start a Jupyter notebook using the following command\n",
    "\n",
    "    `jupyter notebook <path/to/directory>`    \n",
    "    Simply use any of the scripts given above.\n",
    "    \n",
    "5. In IDEs, write your code after the initialization script.\n",
    "\n",
    "#### Linux (Ubuntu)\n",
    "\n",
    "To be updated soon"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
