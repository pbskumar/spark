{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "spark_home_folder = os.environ['SPARK_HOME']\n",
    "sys.path.append(spark_home_folder + r'\\python')\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"Error: \", e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\")\n",
    "conf.setAppName(\"spark_wc\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x90c2ad6f28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# flatMapValues(func)\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "print(\"Original RDD: \")\n",
    "print(rdd.collect())\n",
    "\n",
    "rdd2 = rdd.flatMapValues(lambda x: range(x, x+2)).collect()\n",
    "print(\"After transformation: \")\n",
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9949,udp,other,SF,146,105,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,3,0.01,0.73,0.98,0.00,0.00,0.00,0.00,0.00,normal.']\n",
      "Generated Key-Value pairs:\n",
      "[('warezmaster.', ['9', 'tcp', 'ftp_data', 'SF', '0', '5153771', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '12', '12', '1.00', '0.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', 'warezmaster.'])]\n"
     ]
    }
   ],
   "source": [
    "# Read this file and map each line to a key-value pair\n",
    "# Each line has ',' separated values. Key nust be the 41st item and value must be the list of all items.\n",
    "\n",
    "import urllib.request\n",
    "# f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "# print(raw_data.take(5))\n",
    "\n",
    "print(raw_data.top(1))\n",
    "\n",
    "# Creating Key - Value pairs\n",
    "rdd = raw_data.map(lambda x: (x.split(',')[41], x.split(',')))\n",
    "\n",
    "# Printing generated KV pair\n",
    "print(\"Generated Key-Value pairs:\")\n",
    "print(rdd.top(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3, 2: 4}\n"
     ]
    }
   ],
   "source": [
    "# collectAsMap()\n",
    "# Returns a dictionary of all key value pairs.\n",
    "# If a key has manu values, the key has its latest value in the dictionary\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "print(rdd.collectAsMap())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "# lookup(key)\n",
    "# Returns a list of all the values paired with the given key.\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "print(rdd.lookup(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### aggregateByKey(aggregation_variable, combinine_function, merging_function)\n",
    "\n",
    "aggregateByKey() is used to perform operations on key-value pairs.\n",
    "\n",
    "This can also be achieved by grouping all values of a key and then applying an aggregation function\n",
    "But grouping can be expensive as data needs to be shuffled across nodes.\n",
    "With aggregateByKey() we first perform operations to an get a partial aggregate value in each partition\n",
    "and then use partial values from each partition to compute the final value.\n",
    "This is analogous to combine block in Hadoop MapReduce jobs\n",
    "\n",
    "THe function takes 3 parameters:\n",
    "    1. `aggregateVariable`: initial value of aggregation variable\n",
    "    2. `combining function`: \n",
    "        This function is used to compute partial aggregation value in each partition\n",
    "        Takes 2 input parameters.\n",
    "        a. aggregation variable\n",
    "        b. value from key-value pair\n",
    "    3. `merging function`: \n",
    "        This function is merges partial aggregate values from each partition\n",
    "        Takes 2 input parametes\n",
    "        a. partial value from partition 1\n",
    "        b. partial value from partition 2\n",
    "\n",
    "The following example illustrates the usage of aggregateByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s', 6), ('b', 6), ('a', 5), ('e', 1), ('f', 7), ('d', 8)]\n"
     ]
    }
   ],
   "source": [
    "# aggregateByKey(aggregation_variable, combinine_function, merging_function)\n",
    "\n",
    "\n",
    "# Find the number of occurrences of each letter\n",
    "\n",
    "rdd = sc.parallelize(list(\"aaaaabbbbbbdfdfsssdfdfdsddfsfefds\"))\n",
    "rdd = rdd.map(lambda x: (x,1))\\\n",
    "        .aggregateByKey(\n",
    "                        # Initial value of count for each key\n",
    "                        0,\n",
    "                        # Combining Function: Combines values in each partition\n",
    "                        # c is the count of each key. Initialized to 0 in first argument\n",
    "                        lambda c, x: c + x,\n",
    "                        # Merging Function: COmbines values across partitions\n",
    "                        lambda x, y: x + y\n",
    "                       )\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### combineByKey()\n",
    "\n",
    "combineByKey() is transformation similar to aggregateByKey(). \n",
    "\n",
    "\n",
    "combineByKey is more general then aggregateByKey. aggregateByKey is suitable for compute aggregations for keys, example aggregations such as sum, etc. aggregateByKey() is additional computation after map on local partitions to reduce the amount of data sent out to other nodes and driver. \n",
    "\n",
    "combineByKey is more general and offers the flexibility to specify any map side combine function. \n",
    "\n",
    "Usage:\n",
    "\n",
    "`combineByKey(createCombiner, mergeValue, mergeCombiner)`\n",
    "\n",
    "combineByKey() takes 3 other functions as input parameters.\n",
    "1. `createCombiner`: \n",
    "    This is the very first aggregation step for each key. \n",
    "    All required variables can be initialized here. \n",
    "    Ex: ```python lambda value: (value, 1)```\n",
    "    For avgerage, we need to maintain sum and count variables (sum, count)\n",
    "            \n",
    "            \n",
    "2. `mergeValue`:\n",
    "    Given a new value for a key, this function defines how to manipulate the data structure \n",
    "    created in createCombiner. Takes 2 input parameters. The first parameter is the combiner data structure and\n",
    "    second parameter is the value.\n",
    "    This operation takes place in each partition and partial values are computed.\n",
    "    Ex: ```python lambda x, val: (x[0] + val, x[1] + 1)```\n",
    "    x is the combiner data structure, where x[0] is sum and x[1] is count\n",
    "    val is the new value\n",
    "   \n",
    "3. `mergeCombiner`:\n",
    "    This function defines how to merge combiners. It takes partial combiner values from 2 patitions as inputs. \n",
    "    Each input is a obtained from different partitions and merged together.\n",
    "\n",
    "    Ex: ```python lambda x, y: (x[0] + y[0], x[1] + y[1])```\n",
    "\n",
    "\n",
    "[Refrence](http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('average', 4.0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average of given list of numbers\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7])\n",
    "combiner_rdd = rdd.map(lambda x: ('key', x))\\\n",
    "                .combineByKey(\n",
    "                    lambda val: (val, 1),\n",
    "                    lambda x, val: (x[0] + val, x[1] + 1),\n",
    "                    lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                )\n",
    "    \n",
    "avg_rdd = combiner_rdd.map(lambda x: (\"average\", x[1][0]*1.0/x[1][1]))\n",
    "\n",
    "avg_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### pipe()\n",
    "\n",
    "pipe(cmd) is a transformation function of RDDs which is used to send each element to script `cmd` as input. `cmd` could be any pre-built functions or user-defined functions.\n",
    "\n",
    "The `pipe()` is very helpful to interface Spark program with any other program, irrespective of its language. This can be useful to run legacy software, scripts in other languages, thus enabling code reuse. \n",
    "Examples: \n",
    "1. Executing existing Java code in PySpark script\n",
    "2. Using legacy [BLAST] code to search protein databases parellely.\n",
    "    \n",
    "[BLAST]: http://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastHome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pipeScript.py\n",
    "\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    print(\"The name entered is:  + {}\".format(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 111, in main\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 106, in process\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\rdd.py\", line 715, in func\n    shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)\n  File \"C:\\Users\\PBSKU\\Anaconda3\\lib\\subprocess.py\", line 950, in __init__\n    restore_signals, start_new_session)\n  File \"C:\\Users\\PBSKU\\Anaconda3\\lib\\subprocess.py\", line 1220, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 111, in main\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 106, in process\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\rdd.py\", line 715, in func\n    shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)\n  File \"C:\\Users\\PBSKU\\Anaconda3\\lib\\subprocess.py\", line 950, in __init__\n    restore_signals, start_new_session)\n  File \"C:\\Users\\PBSKU\\Anaconda3\\lib\\subprocess.py\", line 1220, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9606d41afe25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Alice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Bob\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Eve\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresultRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pipeScript'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresultRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \"\"\"\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PBSKU\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PBSKU\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 111, in main\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 106, in process\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\rdd.py\", line 715, in func\n    shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)\n  File \"C:\\Users\\PBSKU\\Anaconda3\\lib\\subprocess.py\", line 950, in __init__\n    restore_signals, start_new_session)\n  File \"C:\\Users\\PBSKU\\Anaconda3\\lib\\subprocess.py\", line 1220, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 111, in main\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 106, in process\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:\\Spark\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\rdd.py\", line 715, in func\n    shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)\n  File \"C:\\Users\\PBSKU\\Anaconda3\\lib\\subprocess.py\", line 950, in __init__\n    restore_signals, start_new_session)\n  File \"C:\\Users\\PBSKU\\Anaconda3\\lib\\subprocess.py\", line 1220, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Script to illustrate functionality of pipe()\n",
    "from pprint import pprint\n",
    "rdd = sc.parallelize([\"Alice\", \"Bob\", \"Eve\"])\n",
    "resultRDD = rdd.pipe('pipeScript').collect()\n",
    "pprint(resultRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
