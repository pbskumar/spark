{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Apache Spark\n",
    "\n",
    "Apache Spark is a fast and general-purpose open source cluster computing framework. Spark has a Core Engine which manages of Distributed task dispatching, scheduling etc, and provides high-level APIs (supports Scala, Python, R) for implementation. \n",
    "\n",
    "The APIs provieded are centered on Sparks's native data structure known as Resilient Distributed Dataset (RDD). RDDs are immutable data-structures which are inherently fault tolerant. RDDs will be described in detail in later sections.\n",
    "\n",
    "Spark requires a cluster manager and a distributed storage system to implement its core funtionalities. Generally, Spark uses HDFS and YARN of Hadoop 2.x to support its distributed storage and cluster management. Spark also supports standalone local cluster (With just one thread). However, it can be used to interface Cassandra, Amazon S3 for storage and Apache Mesos for cluster management. \n",
    "\n",
    "Spark's Python API, PySpark is used to program Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. Install latest version of JAVA.\n",
    "2. Set JAVA_HOME environment variable, pointing to the Java directory\n",
    "    For example, your path could be: C:\\Program Files\\Java\\jdk1.8.0_91\n",
    "3. Download latest Hadoop version from [here](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.1/).\n",
    "4. Set HADOOP_HOME environment variable pointing to Hadoop directory\n",
    "5. Download *winutils.exe* from [here](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin).\n",
    "6. Copy winutils.exe to the bin folder in Hadoop directory (HADOOP_HOME/bi).\n",
    "7. Download Apache Spark pre-built for your Hadoop version from [here](http://spark.apache.org/downloads.html).\n",
    "8. Set SPARK_HOME environment variable pointing to Hadoop directory.\n",
    "9. This is a good time to restart your computer.\n",
    "\n",
    "##### Running Spark\n",
    "10. Now open the command prompt, navigate to SPARK_HOME/bin and execute pyspark.exe\n",
    "    This initializes Spark and the command prompt myust look something like ![this](/images/pyspark_init.jpg).\n",
    "    \n",
    "11. Instead of navigating to SPARK_HOME/bin everytime, it is a good idea to add [SPARK_HOME/bin/pyspark.exe) to the PATH environment variable. This way we can run pyspark from any folder.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
