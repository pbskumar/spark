{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "spark_home_folder = r'C:\\Spark\\spark-1.6.1-bin-hadoop2.6'\n",
    "os.environ['SPARK_HOME'] = spark_home_folder\n",
    "\n",
    "sys.path.append(spark_home_folder + r'\\python')\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"Error: \", e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\")\n",
    "conf.setAppName(\"spark_wc\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1edc286f28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# flatMapValues(func)\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "print(\"Original RDD: \")\n",
    "print(rdd.collect())\n",
    "\n",
    "rdd2 = rdd.flatMapValues(lambda x: range(x, x+2)).collect()\n",
    "print(\"After transformation: \")\n",
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9949,udp,other,SF,146,105,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,3,0.01,0.73,0.98,0.00,0.00,0.00,0.00,0.00,normal.']\n",
      "Generated Key-Value pairs:\n",
      "[('warezmaster.', ['9', 'tcp', 'ftp_data', 'SF', '0', '5153771', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '12', '12', '1.00', '0.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', 'warezmaster.'])]\n"
     ]
    }
   ],
   "source": [
    "# Read this file and map each line to a key-value pair\n",
    "# Each line has ',' separated values. Key nust be the 41st item and value must be the list of all items.\n",
    "\n",
    "import urllib.request\n",
    "# f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "# print(raw_data.take(5))\n",
    "\n",
    "print(raw_data.top(1))\n",
    "\n",
    "# Creating Key - Value pairs\n",
    "rdd = raw_data.map(lambda x: (x.split(',')[41], x.split(',')))\n",
    "\n",
    "# Printing generated KV pair\n",
    "print(\"Generated Key-Value pairs:\")\n",
    "print(rdd.top(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1edc286f28>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combineByKey(createCombiner, MergeValue, MergeCombiner)\n",
    "# \n",
    "# \n",
    "\n",
    "# Re-read carefully\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3, 2: 4}\n"
     ]
    }
   ],
   "source": [
    "# collectAsMap()\n",
    "# Returns a dictionary of all key value pairs.\n",
    "# If a key has manu values, the key has its latest value in the dictionary\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "print(rdd.collectAsMap())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "# lookup(key)\n",
    "# Returns a list of all the values paired with the given key.\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "print(rdd.lookup(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
