{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "spark_home_folder = r'C:\\Spark\\spark-1.6.1-bin-hadoop2.6'\n",
    "os.environ['SPARK_HOME'] = spark_home_folder\n",
    "\n",
    "sys.path.append(spark_home_folder + r'\\python')\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"Error: \", e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\")\n",
    "conf.setAppName(\"spark_wc\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1edc286f28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# flatMapValues(func)\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "print(\"Original RDD: \")\n",
    "print(rdd.collect())\n",
    "\n",
    "rdd2 = rdd.flatMapValues(lambda x: range(x, x+2)).collect()\n",
    "print(\"After transformation: \")\n",
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9949,udp,other,SF,146,105,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,3,0.01,0.73,0.98,0.00,0.00,0.00,0.00,0.00,normal.']\n",
      "Generated Key-Value pairs:\n",
      "[('warezmaster.', ['9', 'tcp', 'ftp_data', 'SF', '0', '5153771', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '12', '12', '1.00', '0.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', 'warezmaster.'])]\n"
     ]
    }
   ],
   "source": [
    "# Read this file and map each line to a key-value pair\n",
    "# Each line has ',' separated values. Key nust be the 41st item and value must be the list of all items.\n",
    "\n",
    "import urllib.request\n",
    "# f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "# print(raw_data.take(5))\n",
    "\n",
    "print(raw_data.top(1))\n",
    "\n",
    "# Creating Key - Value pairs\n",
    "rdd = raw_data.map(lambda x: (x.split(',')[41], x.split(',')))\n",
    "\n",
    "# Printing generated KV pair\n",
    "print(\"Generated Key-Value pairs:\")\n",
    "print(rdd.top(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1edc286f28>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combineByKey(createCombiner, MergeValue, MergeCombiner)\n",
    "# \n",
    "# \n",
    "\n",
    "# Re-read carefully\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3, 2: 4}\n"
     ]
    }
   ],
   "source": [
    "# collectAsMap()\n",
    "# Returns a dictionary of all key value pairs.\n",
    "# If a key has manu values, the key has its latest value in the dictionary\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "print(rdd.collectAsMap())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "# lookup(key)\n",
    "# Returns a list of all the values paired with the given key.\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (1,3), (2,4)])\n",
    "print(rdd.lookup(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### aggregateByKey(aggregation_variable, combinine_function, merging_function)\n",
    "\n",
    "aggregateByKey() is used to perform operations on key-value pairs.\n",
    "\n",
    "This can also be achieved by grouping all values of a key and then applying an aggregation function\n",
    "But grouping can be expensive as data needs to be shuffled across nodes.\n",
    "With aggregateByKey() we first perform operations to an get a partial aggregate value in each partition\n",
    "and then use partial values from each partition to compute the final value.\n",
    "This is analogous to combine block in Hadoop MapReduce jobs\n",
    "\n",
    "THe function takes 3 parameters:\n",
    "    1. initial value of aggregation variable\n",
    "    2. combining function: \n",
    "        This function is used to compute partial aggregation value in each partition\n",
    "        Takes 2 input parameters.\n",
    "        a. aggregation variable\n",
    "        b. value from key-value pair\n",
    "        \n",
    "    3. merging function: \n",
    "        This function is merges partial aggregate values from each partition\n",
    "        Takes 2 input parametes\n",
    "        a. partial value from partition 1\n",
    "        b. partial value from partition 2\n",
    "\n",
    "The following example illustrates the usage of aggregateByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s', 6), ('b', 6), ('a', 5), ('e', 1), ('f', 7), ('d', 8)]\n"
     ]
    }
   ],
   "source": [
    "# aggregateByKey(aggregation_variable, combinine_function, merging_function)\n",
    "\n",
    "\n",
    "# Find the number of occurances of each letter\n",
    "\n",
    "rdd = sc.parallelize(list(\"aaaaabbbbbbdfdfsssdfdfdsddfsfefds\"))\n",
    "rdd = rdd.map(lambda x: (x,1))\\\n",
    "        .aggregateByKey(\n",
    "                        # Initial value of count for each key\n",
    "                        0,\n",
    "                        # Combining Function: Combines values in each partition\n",
    "                        # c is the count of each key. Initialized to 0 in first argument\n",
    "                        lambda c, x: c + x,\n",
    "                        # Merging Function: COmbines values across partitions\n",
    "                        lambda x, y: x + y\n",
    "                       )\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### combineByKey()\n",
    "\n",
    "combineByKey() is transformation similar to aggregateByKey(). \n",
    "\n",
    "\n",
    "combineByKey is more general then aggregateByKey. aggregateByKey is suitable for compute aggregations for keys, example aggregations such as sum, etc. aggregateByKey() is additional computation after map on local partitions to reduce the amount of data sent out to other nodes and driver. \n",
    "\n",
    "combineByKey is more general and offers the flexibility to specify any map side combine function. \n",
    "\n",
    "Usage:\n",
    "\n",
    "combineByKey(createCombiner, mergeValue, mergeCombiner)\n",
    "\n",
    "combineByKey() takes 3 other functions as input parameters.\n",
    "    1. createCombiner: \n",
    "        This is the very first aggregation step for each key. \n",
    "        All required variables can be initialized here. \n",
    "        Ex: lambda value: (value, 1)\n",
    "        For avgerage, we need to maintain sum and count variables (sum, count)\n",
    "            \n",
    "            \n",
    "    2. mergeValue:\n",
    "        Given a new value for a key, this function defines how to manipulate the data structure \n",
    "        created in createCombiner. Takes 2 input parameters. The first parameter is the combiner data structure and\n",
    "        second parameter is the value.\n",
    "        This operation takes place in each partition and partial values are computed.\n",
    "        Ex: lambda x, val: (x[0] + val, x[1] + 1)\n",
    "        x is the combiner data structure, where x[0] is sum and x[1] is count\n",
    "        val is the new value\n",
    "   \n",
    "    3. mergeCombiner:\n",
    "        This function defines how to merge combiners. It takes partial combiner values from 2 patitions as inputs. \n",
    "        Each input is a obtained from different partitions and merged together.\n",
    "        \n",
    "        Ex: lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "\n",
    "Refrence: http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('average', 4.0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average of given list of numbers\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7])\n",
    "combiner_rdd = rdd.map(lambda x: ('key', x))\\\n",
    "                .combineByKey(\n",
    "                    lambda val: (val, 1),\n",
    "                    lambda x, val: (x[0] + val, x[1] + 1),\n",
    "                    lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                )\n",
    "    \n",
    "avg_rdd = combiner_rdd.map(lambda x: (\"average\", x[1][0]*1.0/x[1][1]))\n",
    "\n",
    "avg_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
